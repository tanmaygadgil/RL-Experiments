{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DQN prototyping\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard coded variables\n",
    "EPISODES = 10\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = 0.99\n",
    "SAMPLE_BATCH = 64\n",
    "GAMMA = 0.99\n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = namedtuple(\"experience\", field_names=['s', 'a', 'r','done', 's_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, sample_size = SAMPLE_BATCH):\n",
    "        state = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        state_p = []\n",
    "        ##get sample of indices\n",
    "        if sample_size > len(self.buffer):\n",
    "            \n",
    "            for i in range(len(self.buffer)):\n",
    "                s, a, r, d, s_p = self.buffer[i]\n",
    "                state.append(s)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "                dones.append(d)\n",
    "                state_p.append(s_p)\n",
    "            return np.array(state), np.array(actions), np.array(rewards), np.array(dones), np.array(state_p)\n",
    "        else:\n",
    "            indices = np.random.choice(len(self.buffer), \n",
    "                                       sample_size, \n",
    "                                       replace=False)\n",
    "            for i in indices:\n",
    "                s, a, r, d, s_p = self.buffer[i]\n",
    "                state.append(s)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "                dones.append(d)\n",
    "                state_p.append(s_p)\n",
    "            return np.array(state), np.array(actions), np.array(rewards), np.array(dones), np.array(state_p)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(keras.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = keras.layers.Dense(64)\n",
    "        self.layer2 = keras.layers.Dense(128)\n",
    "        self.layer3 = keras.layers.Dense(128)\n",
    "        self.layer4 = keras.layers.Dense(action_size)\n",
    "        \n",
    "    def call(self, input):\n",
    "        x = self.layer1(input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(EPSILON):\n",
    "    \n",
    "    try:\n",
    "        model = Model(env.action_space.n)\n",
    "\n",
    "        model.compile(loss=tf.losses.CategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics = tf.keras.metrics.CategoricalAccuracy())\n",
    "        replay = ExperienceReplay(10000)\n",
    "        best_reward = 0\n",
    "        scores = []\n",
    "        for episode in range(EPISODES):\n",
    "\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "\n",
    "            while True:\n",
    "\n",
    "                ## Choose an action\n",
    "                if np.random.random() > EPSILON:\n",
    "                    action = model.predict(state.reshape(1, -1))\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                env.render()\n",
    "                state_p, reward, done, _ = env.step(action=action)\n",
    "                replay.add(Exp(s = state, \n",
    "                               a = action,\n",
    "                               r = reward,\n",
    "                               done= done, \n",
    "                               s_p = state_p))\n",
    "                #Learn \n",
    "                states, actions, rewards, dones, states_p = replay.sample()\n",
    "                q_vals = model.predict(states)\n",
    "                print(q_vals.shape)\n",
    "                print(actions)\n",
    "                q_vals_new = np.max(model.predict(states_p), axis = 1)\n",
    "\n",
    "                q_target = rewards + GAMMA * q_vals_new * dones\n",
    "                print(q_target.shape)\n",
    "                print(states.shape)\n",
    "                model.fit(states, q_target, epochs = 1)\n",
    "\n",
    "                EPSILON *= EPSILON_DECAY\n",
    "                state = state_np\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    if score > best_reward:\n",
    "                        best_reward = score\n",
    "                    print(\"Episode {}  Best Reward {} Last Reward {} Epsilon {}\"\\\n",
    "                          .format(i, best_reward, score, EPSILON))\n",
    "                    break\n",
    "\n",
    "            scores.append[score]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "            \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(1,)\n",
      "(1, 8)\n",
      "in user code:\n",
      "\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n",
      "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n",
      "        return self.compiled_loss(\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n",
      "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n",
      "        losses = call_fn(y_true, y_pred)\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n",
      "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n",
      "        return backend.categorical_crossentropy(\n",
      "    File \"C:\\Users\\tanma\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n",
      "        target.shape.assert_is_compatible_with(output.shape)\n",
      "\n",
      "    ValueError: Shapes (None, 1) and (None, 4) are incompatible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dqn(EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.random.random((100,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [np.random.randint(0,3) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = m.predict(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00389986,  1.391949  , -0.20417786, -0.43419787,  0.00670153,\n",
       "         0.0897917 ,  0.        ,  0.        ], dtype=float32),\n",
       " -2.2680782152933134,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.06623831,  0.9630391 , -0.25177294, -0.9925629 ,  0.21233341,\n",
       "         0.09543299,  0.        ,  0.        ], dtype=float32),\n",
       " -0.48689421732714666,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(env.reset().reshape(1,-1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.random.random((10,8))\n",
    "rewards = np.random.random((10,1))\n",
    "dones = np.array([np.random.choice([True, False]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "q_fut = np.max(model.predict(state), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40931207, 0.09657527, 0.90224723, 0.31664349, 0.51111311,\n",
       "       1.25086158, 0.32520274, 1.35936309, 0.42618603, 0.67265275])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones*q_fut*GAMMA +rewards.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31978217, 0.08766681, 0.42509478, 0.30133817, 0.30999145,\n",
       "       0.2773019 , 0.38368082, 0.36521935, 0.26070544, 0.1978873 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = np.array([])\n",
    "o = np.append(o, np.array([1,2,3]))\n",
    "o = np.append(o, np.array([4,5,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = [[1,2,3],[1,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e998891dc8591c319a10683017fd813ece281029df2a663879d0bcabaac420"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
